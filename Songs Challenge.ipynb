{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = pd.read_csv('album_details.csv')\n",
    "albums.drop(columns=['Unnamed: 0','id','name','type','year'], inplace = True)\n",
    "albums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('songs_details.csv')\n",
    "songs.drop(columns=['Unnamed: 0','song_href','song_id'], inplace = True)\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv('lyrics.csv')\n",
    "lyrics.drop(columns=['Unnamed: 0','link'], inplace = True)\n",
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets choose the top two singers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums.singer_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = albums[(albums.singer_name == 'Johnny Cash Lyrics') | (albums.singer_name == 'Frank Sinatra Lyrics')]\n",
    "albums = albums.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's get the songs of the two singers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_songs = pd.merge(albums, songs, how=\"left\", on=[\"singer_name\"])\n",
    "albums_songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the lyrics? Notice that singer name column is named 'artist' in lyrics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics.rename(columns={'artist': \"singer_name\"}, inplace = True)\n",
    "albums_songs = pd.merge(albums_songs, lyrics, how=\"left\", on=[\"singer_name\",\"song_name\"])\n",
    "albums_songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check how many nulls and duplicates we got and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of nulls in albums_songs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicates in albums_songs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls and duplicates. Don't forget to reset index\n",
    "albums_songs = \n",
    "# some singers have a common song so for this exercise we will remove all common songs\n",
    "albums_songs = albums_songs[~albums_songs.lyrics.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lyrics text Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the lyrics on the one hand and the singer on the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric = albums_songs.lyrics\n",
    "singer = albums_songs.singer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a lyric to see what it looks like\n",
    "print(lyric[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the text cleaning function:\n",
    "Thiss function will clean the text to be able to continue with text processing with TF-IDF.\n",
    "For this function we recommend using the re package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   \n",
    "import nltk\n",
    "nltk.download('stopwords')  # you have to download it the first time\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleanText(lyric):   \n",
    "    # Remove the expressions between braces that denote the parts of the song\n",
    "\n",
    "    # Remove clarifications in parentheses\n",
    "\n",
    "    # Remove what are not words or something that resembles it\n",
    "\n",
    "    # Remove extra spaces if there are\n",
    "\n",
    "    # Convert the text to lowercase\n",
    "    \n",
    "    # remove stopwords\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the function to our set of lyrics\n",
    "clean_lyric = lyric.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the lyric to see the changes\n",
    "print(clean_lyric[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the lemmatization function\n",
    "\n",
    "def lemmatizeText(text):   \n",
    "    #apply lemmatization\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stemming function\n",
    "\n",
    "def stemText(text):   \n",
    "    #apply stemming\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets see the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with neither lemmatization nor stemming\n",
    "print(clean_lyric[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with lemmatization only\n",
    "print(lemmatizeText(clean_lyric[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stemming only\n",
    "print(stemText(clean_lyric[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you see the differencec? Now lets choose one of them to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose either lemmatization or stemming for the text before going into Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw a wordcloud of one of the lyrics texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = #call the wordcloud function\n",
    "fig = plt.figure(1, figsize=(20, 20))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try for all the lyric texts together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = #call the wordcloud function\n",
    "fig = plt.figure(1, figsize=(20, 20))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you guess the most important terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to transform our clean lyrics texts into matrix form using TfidfVectorizer from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Tf-IDF for the cleaned lyric texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get the top terms in the lyric texts using the tf-idf matric we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = np.array(tfidfconverter.get_feature_names())\n",
    "tfidf_sorting = np.argsort(X).flatten()[::-1]\n",
    "n = 5\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who missed the clustering session? Lets try some clusters here.\n",
    "Lets see how many clusters we will need by... do you remember the elbow method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "scores = []\n",
    "k_values = range(2,15)\n",
    "for a in k_values:\n",
    "    # call kmeans for each clusters number and append its score\n",
    "\n",
    "sns.lineplot(x=k_values, y=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a clusters number and apply kmeans \n",
    "kmeans = \n",
    "\n",
    "# Prepare the factorplot\n",
    "cluster_result = pd.DataFrame()\n",
    "cluster_result['cluster'] = kmeans.predict(X)\n",
    "cluster_result['Actual'] = singer\n",
    "\n",
    "ax = sns.factorplot(col='cluster', x='Actual', data=cluster_result, kind='count',col_wrap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets plot the cluster data. Oh wait we have many features, so how can we visualize them in 2-D? Can you remember? Hint: something to do with dimensionality reduction\n",
    "We need two plots: 1. A colored scatterplot with the actual labels for all the data together. 2. A scatterplot with the number of clusters that we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dimensionality reduction\n",
    "\n",
    "#below are the two plots but you need to add x, y, and hue\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(15,10))\n",
    "plot1 = sns.scatterplot(x = , y =, hue = , legend = \"full\", ax = ax1)\n",
    "plot2 = sns.scatterplot(x = , y =, hue = , legend = \"full\", ax = ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are the clusters? Can you see them? Kmeans didn't help this time, so maybe a classifier can help us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: splitting the data between text and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, singer, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train a classifier and get its accuracy. Do you remember what classifier we took in session 4 (Alican)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a classifier. Rememeber you need to fit the classifier into the train data and score it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
